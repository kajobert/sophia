# SOPHIA Autonomy & Safety Roadmap

**Date:** 2025-11-07  
**Focus:** BezpeÄnÃ© a spolehlivÃ© self-improvement pro Sophii  
**Priority:** CRITICAL - Foundation before features

---

## ðŸŽ¯ HlavnÃ­ cÃ­l

> "Sophie musÃ­ bÃ½t schopna **bezpeÄnÄ› a spolehlivÄ›** se zdokonalovat a upgradovat **dÅ™Ã­ve** neÅ¾ testuje dashboard nebo provÃ¡dÃ­ jinÃ© Ãºkoly."

### SouÄasnÃ½ stav (z AMI_TODO_ROADMAP.md):

**âœ… Co funguje:**
- Self-tuning plugin (cognitive_self_tuning.py - 1,437 lines)
- Hypothesis generation (cognitive_reflection.py)
- Sandbox testing environment
- Benchmark system (pytest integration)
- Automatic rollback on failure
- Git commit automation
- GitHub PR creation (Phase 3.5)
- Autonomous restart & validation (Phase 3.7)

**âš ï¸ CO CHYBÃ - KRITICKÃ‰ MEZERY:**

---

## ðŸš¨ CRITICAL GAPS - Must Fix First

### 1ï¸âƒ£ **BezpeÄnostnÃ­ validace PÅ˜ED deploymentem**

**ProblÃ©m:**
```python
# cognitive_self_tuning.py souÄasnÄ›:
if improvement_pct >= self.improvement_threshold:
    # âœ… Benchmark passed â†’ DEPLOY!
    # âŒ ALE: Å½Ã¡dnÃ¡ validace bezpeÄnosti!
    # âŒ Co kdyÅ¾ fix obsahuje:
    #    - MazÃ¡nÃ­ dÅ¯leÅ¾itÃ½ch souborÅ¯?
    #    - NekoneÄnÃ½ loop?
    #    - SQL injection?
    #    - API klÃ­Äe v kÃ³du?
```

**ChybÃ­:**
- âŒ Static code analysis (ruff, mypy, bandit)
- âŒ Security scan (detekce secrets, SQL injection, path traversal)
- âŒ Syntax validation (Python AST parsing)
- âŒ Import validation (dependencies check)
- âŒ Complexity analysis (McCabe, cyclomatic complexity)

**Å˜eÅ¡enÃ­:** **cognitive_code_validator.py** (NEW)

---

### 2ï¸âƒ£ **Test coverage validace**

**ProblÃ©m:**
```python
# Sophie mÅ¯Å¾e deployovat kÃ³d kterÃ½:
# âœ… MÃ¡ benchmark improvement +15%
# âŒ ALE: NemÃ¡ Å½ÃDNÃ‰ testy!
# âŒ Nebo: Testy nebÄ›Å¾Ã­ (import errors)
# âŒ Nebo: Testy jsou fake (always pass)
```

**ChybÃ­:**
- âŒ Pytest execution PÅ˜ED deployment
- âŒ Code coverage measurement (pytest-cov)
- âŒ Test quality validation (assertions check)
- âŒ Integration test execution

**Å˜eÅ¡enÃ­:** Extend **cognitive_self_tuning.py** s test validation

---

### 3ï¸âƒ£ **Rollback testing**

**ProblÃ©m:**
```python
# Sophie mÃ¡ rollback mechanismus, ALE:
# âŒ Nikdy netestujeme Å¾e rollback FUNGUJE!
# âŒ Co kdyÅ¾ backup je corrupted?
# âŒ Co kdyÅ¾ restore selÅ¾e?
```

**ChybÃ­:**
- âŒ Periodic rollback drills (mÄ›sÃ­ÄnÃ­ test)
- âŒ Backup integrity validation (hash check)
- âŒ Restore simulation (v sandboxu)

**Å˜eÅ¡enÃ­:** **cognitive_reliability_monitor.py** (NEW)

---

### 4ï¸âƒ£ **Hypothesis kvalita**

**ProblÃ©m:**
```python
# cognitive_reflection.py generuje hypotheses, ALE:
# âŒ Å½Ã¡dnÃ¡ validace Å¾e hypothesis je IMPLEMENTOVATELNÃ
# âŒ MÅ¯Å¾e vygenerovat: "PÅ™epiÅ¡ kernel do Rust" 
# âŒ MÅ¯Å¾e vygenerovat: "PouÅ¾ij GPT-5" (neexistuje)
```

**ChybÃ­:**
- âŒ Feasibility analysis (je to vÅ¯bec moÅ¾nÃ©?)
- âŒ Scope validation (nenÃ­ to moc velkÃ©?)
- âŒ Dependencies check (mÃ¡me potÅ™ebnÃ© tools?)
- âŒ Risk assessment (high/medium/low risk?)

**Å˜eÅ¡enÃ­:** Extend **cognitive_reflection.py** s hypothesis validation

---

### 5ï¸âƒ£ **Production monitoring**

**ProblÃ©m:**
```python
# Po deployment:
# âœ… Git commit created
# âœ… Hypothesis status = "deployed_awaiting_validation"
# âŒ ALE: Jak Sophie zjistÃ­ Å¾e deployment OPRAVDU funguje v produkci?
# âŒ Å½Ã¡dnÃ½ monitoring degradace performance
# âŒ Å½Ã¡dnÃ½ monitoring error rate
```

**ChybÃ­:**
- âŒ Post-deployment monitoring (7 dnÃ­)
- âŒ Performance regression detection
- âŒ Error rate tracking
- âŒ Automatic rollback on production issues

**Å˜eÅ¡enÃ­:** **cognitive_deployment_monitor.py** (NEW)

---

### 6ï¸âƒ£ **Human approval pro kritickÃ© zmÄ›ny**

**ProblÃ©m:**
```python
# Sophie mÅ¯Å¾e deployovat COKOLIV pokud benchmark +10%, INCLUDING:
# - ZmÄ›na kernelu (core/kernel.py)
# - ZmÄ›na event systÃ©mu (core/events.py)
# - ZmÄ›na databÃ¡ze (core/memory_sqlite.py)
# - ZmÄ›na security (authentication, API keys)
```

**ChybÃ­:**
- âŒ Whitelist/blacklist critical files
- âŒ Human approval workflow pro critical changes
- âŒ Deployment permissions system
- âŒ Change categorization (safe/risky/critical)

**Å˜eÅ¡enÃ­:** **Safety config** + approval workflow

---

### 7ï¸âƒ£ **Observability & Auditability**

**ProblÃ©m:**
```python
# Po deployment:
# âŒ TÄ›Å¾kÃ© zjistit CO pÅ™esnÄ› Sophie zmÄ›nila
# âŒ TÄ›Å¾kÃ© zjistit PROÄŒ to zmÄ›nila
# âŒ TÄ›Å¾kÃ© zjistit JAK to testovala
# âŒ Å½Ã¡dnÃ½ audit trail
```

**ChybÃ­:**
- âŒ Detailed deployment logs
- âŒ Hypothesis decision trail (WHY approved/rejected)
- âŒ Benchmark result archival
- âŒ Change impact analysis
- âŒ Dashboard deployment view

**Å˜eÅ¡enÃ­:** Enhanced logging + Dashboard deployment tab

---

## ðŸ“‹ PRIORITY ROADMAP - Security First

### **PHASE A: Safety Foundation** ðŸ”´ CRITICAL

**Timeline:** 1-2 days  
**Goal:** Prevent Sophie from breaking production

#### A.1: Code Validator Plugin âš ï¸ HIGHEST PRIORITY
**File:** `plugins/cognitive_code_validator.py` (NEW - ~400 lines)

**Features:**
```python
class CognitiveCodeValidator:
    def validate_code_change(self, file_path, new_code):
        """Multi-layer validation before deployment."""
        
        # Layer 1: Syntax validation
        try:
            ast.parse(new_code)
        except SyntaxError:
            return {"valid": False, "reason": "Syntax error"}
        
        # Layer 2: Security scan
        issues = self._security_scan(new_code)
        if issues["high_risk"]:
            return {"valid": False, "reason": f"Security: {issues}"}
        
        # Layer 3: Import validation
        missing = self._check_imports(new_code)
        if missing:
            return {"valid": False, "reason": f"Missing deps: {missing}"}
        
        # Layer 4: Complexity check
        complexity = self._check_complexity(new_code)
        if complexity > 15:  # McCabe threshold
            return {"valid": False, "reason": "Too complex"}
        
        # Layer 5: Critical file check
        if self._is_critical_file(file_path):
            return {"valid": False, "reason": "Requires human approval"}
        
        return {"valid": True}
```

**Integration:**
```python
# cognitive_self_tuning.py - BEFORE deployment:
validator = all_plugins.get("cognitive_code_validator")
validation = validator.validate_code_change(target_file, new_code)

if not validation["valid"]:
    self.logger.error(f"âŒ Validation failed: {validation['reason']}")
    self._update_hypothesis_status(hyp_id, "rejected", validation["reason"])
    return
```

**Security checks:**
- âœ… Syntax validation (AST parsing)
- âœ… Import validation (all dependencies available)
- âœ… Secret detection (API keys, passwords in code)
- âœ… SQL injection patterns
- âœ… Path traversal detection
- âœ… Command injection patterns
- âœ… Complexity analysis (McCabe)
- âœ… Critical file protection

---

#### A.2: Test Coverage Enforcement
**File:** `plugins/cognitive_self_tuning.py` (MODIFY)

**Add before deployment:**
```python
def _run_test_suite(self, target_file: Path) -> Dict[str, Any]:
    """Run pytest with coverage for changed file."""
    
    # Find test file
    test_file = self._find_test_file(target_file)
    if not test_file:
        return {"passed": False, "reason": "No test file found"}
    
    # Run pytest with coverage
    result = subprocess.run(
        [
            "pytest",
            str(test_file),
            f"--cov={target_file.stem}",
            "--cov-report=json",
            "--json-report",
        ],
        capture_output=True,
        timeout=60
    )
    
    # Parse results
    if result.returncode != 0:
        return {"passed": False, "reason": "Tests failed"}
    
    # Check coverage
    coverage_data = json.loads(Path(".coverage.json").read_text())
    coverage_pct = coverage_data["totals"]["percent_covered"]
    
    if coverage_pct < 80:
        return {"passed": False, "reason": f"Coverage too low: {coverage_pct}%"}
    
    return {"passed": True, "coverage": coverage_pct}
```

**Threshold:**
- Minimum 80% coverage pro novÃ© zmÄ›ny
- All tests must pass
- No test timeouts

---

#### A.3: Critical File Protection
**File:** `config/autonomy.yaml` (MODIFY)

**Add safety config:**
```yaml
self_tuning:
  # Existing config...
  
  safety:
    # Files that REQUIRE human approval
    critical_files:
      - "core/kernel.py"
      - "core/event_bus.py"
      - "core/event_loop.py"
      - "core/memory_sqlite.py"
      - "config/autonomy.yaml"
      - "guardian.py"
      - "run.py"
    
    # Files that are FORBIDDEN from auto-deployment
    forbidden_files:
      - ".env"
      - "*.key"
      - "*.pem"
      - "*.crt"
    
    # Maximum allowed complexity (McCabe)
    max_complexity: 15
    
    # Minimum test coverage for deployment
    min_test_coverage: 80
    
    # Deployment approval workflow
    require_approval_for:
      - risk_level: "high"
      - file_category: "critical"
      - complexity: ">15"
      - coverage: "<80%"
```

---

### **PHASE B: Reliability Monitoring** ðŸŸ¡ HIGH PRIORITY

**Timeline:** 1 day  
**Goal:** Detect issues AFTER deployment

#### B.1: Deployment Monitor Plugin
**File:** `plugins/cognitive_deployment_monitor.py` (NEW - ~300 lines)

**Features:**
```python
class CognitiveDeploymentMonitor:
    def monitor_deployment(self, hypothesis_id: str, deployment_time: datetime):
        """Monitor deployment for 7 days, auto-rollback on issues."""
        
        # Collect baseline metrics (before deployment)
        baseline = {
            "error_rate": self._get_error_rate(days=7),
            "avg_latency": self._get_avg_latency(days=7),
            "task_success_rate": self._get_task_success_rate(days=7)
        }
        
        # Monitor for 7 days
        for day in range(7):
            await asyncio.sleep(86400)  # 24 hours
            
            current = {
                "error_rate": self._get_error_rate(days=1),
                "avg_latency": self._get_avg_latency(days=1),
                "task_success_rate": self._get_task_success_rate(days=1)
            }
            
            # Check for regression
            if current["error_rate"] > baseline["error_rate"] * 1.5:
                self.logger.error(f"ðŸš¨ Error rate increased 50%!")
                await self._trigger_rollback(hypothesis_id, "error_rate_spike")
                return
            
            if current["task_success_rate"] < baseline["task_success_rate"] * 0.9:
                self.logger.error(f"ðŸš¨ Task success rate dropped 10%!")
                await self._trigger_rollback(hypothesis_id, "success_rate_drop")
                return
        
        # All good â†’ mark as validated
        self._update_hypothesis_status(hypothesis_id, "deployed_validated")
```

**Metrics tracked:**
- Error rate (from logs)
- Task success rate (from task queue)
- Average latency (from benchmarks)
- Memory usage (from system)
- CPU usage (from system)

---

#### B.2: Rollback Testing
**File:** `plugins/cognitive_reliability_monitor.py` (NEW - ~200 lines)

**Monthly rollback drill:**
```python
async def monthly_rollback_drill(self):
    """Test rollback mechanism monthly."""
    
    # 1. Create fake hypothesis
    # 2. Deploy fake change to sandbox
    # 3. Trigger rollback
    # 4. Verify restore worked
    # 5. Report results
    
    if not rollback_successful:
        self.logger.error("ðŸš¨ ROLLBACK MECHANISM BROKEN!")
        self.event_bus.publish(Event(
            EventType.CRITICAL_FAILURE,
            data={"issue": "rollback_drill_failed"}
        ))
```

---

### **PHASE C: Observability** ðŸŸ¢ MEDIUM PRIORITY

**Timeline:** 1 day  
**Goal:** Sophie i ÄlovÄ›k rozumÃ­ deploymentÅ¯m

#### C.1: Deployment Dashboard Tab
**File:** `frontend/dashboard.html` (MODIFY)

**Add tab:**
```html
<button class="tab" onclick="showTab('deployments')">Deployments</button>

<div id="deployments" class="tab-content">
  <table>
    <tr>
      <th>Date</th>
      <th>Hypothesis</th>
      <th>File</th>
      <th>Status</th>
      <th>Improvement</th>
      <th>Actions</th>
    </tr>
    <!-- Populated via /api/deployments -->
  </table>
</div>
```

**API endpoint:**
```python
@app.get("/api/deployments")
async def get_deployments():
    # Query hypotheses with status = deployed_*
    # Return deployment history
```

---

#### C.2: Enhanced Logging
**File:** `plugins/cognitive_self_tuning.py` (MODIFY)

**Add detailed logs:**
```python
# Before deployment
self.logger.info(f"ðŸ“¦ DEPLOYMENT PLAN:")
self.logger.info(f"   Hypothesis: {hypothesis['hypothesis']}")
self.logger.info(f"   File: {target_file}")
self.logger.info(f"   Improvement: +{improvement_pct}%")
self.logger.info(f"   Validation: {validation}")
self.logger.info(f"   Tests: {test_results}")
self.logger.info(f"   Risk: {risk_level}")

# After deployment
self.logger.info(f"âœ… DEPLOYED:")
self.logger.info(f"   Commit: {commit_sha}")
self.logger.info(f"   Backup: {backup_file}")
self.logger.info(f"   Monitoring: 7 days")
```

---

## ðŸŽ¯ Implementation Priority

### Week 1: Safety Foundation (MUST HAVE)
1. **Day 1-2:** cognitive_code_validator.py
   - Security scanning
   - Critical file protection
   - Syntax validation
   
2. **Day 3:** Test coverage enforcement
   - Pytest integration
   - 80% coverage requirement
   
3. **Day 4:** Safety config
   - autonomy.yaml updates
   - Critical files whitelist

### Week 2: Reliability (SHOULD HAVE)
4. **Day 5-6:** cognitive_deployment_monitor.py
   - Post-deployment monitoring
   - Auto-rollback on issues
   
5. **Day 7:** Rollback testing
   - Monthly drill automation

### Week 3: Observability (NICE TO HAVE)
6. **Day 8:** Dashboard deployment tab
7. **Day 9:** Enhanced logging

---

## âœ… Success Criteria

**Sophie je SAFE kdyÅ¾:**
- âœ… Å½Ã¡dnÃ½ deployment bez security validation
- âœ… Å½Ã¡dnÃ½ deployment bez test coverage â‰¥80%
- âœ… KritickÃ© soubory vyÅ¾adujÃ­ human approval
- âœ… Post-deployment monitoring 7 dnÃ­
- âœ… Automatic rollback on production issues
- âœ… Monthly rollback drill passes
- âœ… Full audit trail v dashboard

**Sophie je RELIABLE kdyÅ¾:**
- âœ… Zero production breakages v poslednÃ­ch 30 dnech
- âœ… Zero failed rollbacks v poslednÃ­ch 90 dnech
- âœ… 100% test pass rate pÅ™ed deploymentem
- âœ… <5% rollback rate (95%+ successful deployments)

---

## ðŸ“š Testing Strategy

**Before enabling auto-deployment:**
1. **Unit tests** pro vÅ¡echny novÃ© pluginy
2. **Integration test** pro deployment workflow
3. **Chaos test** - zÃ¡mÄ›rnÄ› Å¡patnÃ© hypotheses
4. **Rollback test** - ovÄ›Å™enÃ­ Å¾e rollback funguje
5. **Security test** - pokus o deployment dangerous code

**Continuous testing:**
- Daily: Rollback mechanism health check
- Weekly: Full deployment workflow test
- Monthly: Rollback drill

---

**Status:** DRAFT - Waiting for approval  
**Author:** GitHub Copilot  
**Date:** 2025-11-07
