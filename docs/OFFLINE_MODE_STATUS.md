# Offline Mode Implementation Status

**Date:** 2025-11-04  
**Status:** ðŸŸ¡ **Phase 1 Complete - Integration Working, Needs Prompt Tuning**

## âœ… Completed Milestones

### 1. Infrastructure (100% Complete)
- [x] `--offline` flag added to run.py
- [x] `offline_mode` field in SharedContext
- [x] Context propagation through kernel â†’ plugins
- [x] Runtime LLM selection in cognitive_planner
- [x] Runtime LLM selection in cognitive_task_router

### 2. tool_local_llm Implementation (100% Complete)
- [x] **execute() method** fully implemented
  - Extracts prompt from context.payload
  - Loads sophia_dna.txt system prompt
  - Builds messages from context.history
  - Calls generate() â†’ _generate_ollama()
  - Stores result in context.payload["llm_response"]
- [x] **setup() enhanced** to load system prompt
- [x] Compatible interface with tool_llm (drop-in replacement)

### 3. Verification Tests (100% Complete)
- [x] Ollama API direct test: `curl` returns responses âœ…
- [x] Ollama models verified: llama3.1:8b (4.9GB) available âœ…
- [x] tool_local_llm.generate() test: Returns Czech responses âœ…
- [x] Full Sophia offline mode test: Planner calls local LLM âœ…

## ðŸŸ¡ Current Blockers

### Issue 1: Function Calling Format
**Problem:** Llama 3.1 8B struggles with OpenAI function calling format  
**Evidence:** Planner hangs/timeouts when requesting structured tool usage  
**Root Cause:** Local models (even 8B) can't reliably format function calls  
**User Warning:** "jen 'lepÅ¡Ã­' modely dokÃ¡Å¾ou sprÃ¡vnÄ› vracet plÃ¡ny a dobÅ™e formÃ¡tovat"  
**Solution Path:** Iterative prompt tuning (like with OpenRouter models)

### Issue 2: Slow Startup
**Problem:** Sophia takes 30+ seconds to initialize  
**Components:**
- Chroma: ~5s loading
- LiteLLM: ~6s loading
- Other plugins: ~10s
**Impact:** Timeout before Ollama response arrives  
**Priority:** Medium (doesn't block offline functionality, just UX)

## ðŸŽ¯ Next Steps

### Phase 2: Prompt Engineering (Current)
Following user's proven process with OpenRouter:

1. **Test simple tasks** â†’ observe LLM responses
2. **Adjust planner prompts** for local model capabilities
3. **Tune kernel robustness** to handle various response formats
4. **Iterate** until one task works end-to-end

**First Goal:** Get `--once "Ahoj"` to return response without planning

**Approaches:**
- **Option A:** Disable function calling in offline mode (simpler)
- **Option B:** Custom planning format for local models
- **Option C:** Skip planner entirely in offline mode (direct LLM)

### Phase 3: Multi-Model Selection
- Come back to model_strategy.yaml optimization
- Only after **one local model works reliably**

## ðŸ“‹ Implementation Details

### Files Modified

**core/context.py** (Line ~34)
```python
offline_mode: bool = False
```

**run.py** (Line ~192)
```python
context = SharedContext(..., offline_mode=args.offline)
```

**plugins/cognitive_planner.py** (Lines ~56-75)
```python
if context.offline_mode:
    llm_tool = self.plugins.get("tool_local_llm")
    context.logger.info("ðŸ”’ Planner using local LLM (offline mode)")
else:
    llm_tool = self.llm_tool
    context.logger.debug("â˜ï¸ Planner using cloud LLM (online mode)")
```

**plugins/cognitive_task_router.py** (Lines ~87-109)
```python
if context.offline_mode:
    llm_tool = self.plugins.get("tool_local_llm")
else:
    llm_tool = self.plugins.get("tool_llm")
```

**plugins/tool_local_llm.py** (Major Changes)

**setup()** - Added system prompt loading:
```python
self.system_prompt = "You are Sophia, a helpful AI assistant."
try:
    with open("config/prompts/sophia_dna.txt", "r", encoding="utf-8") as f:
        self.system_prompt = f.read()
        logger.info("System prompt loaded from sophia_dna.txt")
except FileNotFoundError:
    logger.warning("sophia_dna.txt not found - using default system prompt")
```

**execute()** - Full implementation (Lines 84-145):
```python
async def execute(self, context: SharedContext) -> SharedContext:
    prompt = context.payload.get("prompt", context.user_input)
    
    # Build messages from history
    messages = []
    if self.system_prompt:
        messages.append({"role": "system", "content": self.system_prompt})
    messages.extend(context.history)
    
    # Combine messages into single prompt
    full_prompt = "\n\n".join([
        f"{msg['role'].upper()}: {msg['content']}" 
        for msg in messages
    ])
    
    response_text = await self.generate(
        prompt=full_prompt,
        temperature=self.config.temperature,
        max_tokens=self.config.max_tokens
    )
    
    context.payload["llm_response"] = response_text
    return context
```

### Helper Scripts Created

**sophia-offline.sh** - Clean offline mode
```bash
python3 run.py --offline "$@"
```

**sophia-offline-debug.sh** - Offline with debug logs
```bash
LOG_LEVEL=DEBUG python3 run.py --offline --debug "$@"
```

## ðŸ”¬ Test Results

### Direct Ollama Test
```bash
$ curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"llama3.1:8b","prompt":"2+2","stream":false}'
# Response: "4"
# Status: âœ… SUCCESS
```

### tool_local_llm.generate() Test
```python
response = await tool.generate(prompt="Ahoj! Kdo jsi?")
# Response: "Obvykle je to ÄlovÄ›k, kterÃ½ se jmenuje Petr. 
#            Jsem ale vÄ›tÅ¡Ã­ neÅ¾ on, protoÅ¾e jsem umÄ›lÃ¡ inteligence."
# Status: âœ… SUCCESS (Czech response!)
```

### Full Sophia Offline Test
```bash
$ bash sophia-offline-debug.sh --once "Ahoj"
# Logs show:
# - âœ… Planner detects offline mode
# - âœ… Calls tool_local_llm.execute()
# - âœ… execute() calls generate()
# - âœ… generate() calls _generate_ollama()
# - âœ… HTTP POST to localhost:11434 initiated
# - âŒ Timeout before response (30s limit)
# Status: ðŸŸ¡ INTEGRATION WORKS but needs prompt tuning
```

## ðŸ§  User's Critical Insights

> "mÃ¡me multi model system kde se mÃ¡ volit dle slozitosti ukolu...  
> je potreba nejprve zprovoznit aspon jeden model"

**Translation:** We have multi-model selection system, but first need to get ONE model working.

> "jen 'lepÅ¡Ã­' modely dokÃ¡Å¾ou sprÃ¡vnÄ› vracet plÃ¡ny a dobÅ™e formÃ¡tovat.  
> ale stejnÄ› jsme dlouho ladili kernel aby si s odpovedÃ­ od LLM poradil co nejrobustnÄ›ji.  
> DÄ›lali jsme to tak Å¾e jsme zkouseli davat ukoly a sledovaly odpovedi od LLM  
> dokud neodpovidali spravne a museli jsme ladit prompty i core."

**Translation:** Only better models can format plans correctly. We still had to tune the kernel for robustness. The process was: try tasks â†’ watch LLM responses â†’ tune prompts AND core â†’ repeat.

**Key Takeaway:** This is NOT a quick fix. Need iterative testing and tuning.

## ðŸ“š References

**Configuration:**
- `config/settings.yaml` - Line 16: `model: "llama3.1:8b"`
- `config/settings.yaml` - Lines 11-20: Local LLM config

**Logs:**
- Use `sophia-offline-debug.sh` to see full execution trace
- Planner logs: "ðŸ”’ Planner using local LLM (offline mode)"
- Local LLM logs: "ðŸ¤– Calling Ollama: model=llama3.1:8b"

**Known Issues:**
- Function calling format incompatible with Llama 3.1 8B
- Slow startup (30s) masks timeout issues
- Need prompt engineering iteration process

---

**Next Session TODO:**
1. Try disabling planner in offline mode (simpler path)
2. Test direct LLM response without function calling
3. If works â†’ gradually add structure back
4. Document what prompt formats work with Llama 3.1 8B
